{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Dataset/data.csv', delimiter=\"@@@\", skiprows=2, encoding='utf-8', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "okgo = pd.read_csv('../Dataset/OKGO.csv', delimiter=\";\", skiprows=2, encoding='latin-1', engine='python') \n",
    "trump = pd.read_csv('../Dataset/trump.csv', delimiter=\",\", skiprows=2, encoding='utf-8', engine='python')\n",
    "swift = pd.read_csv('../Dataset/TaylorSwift.csv', delimiter=\",\", skiprows=2, nrows=180, encoding='utf-8', engine='python')\n",
    "royal = pd.read_csv('../Dataset/RoyalWedding.csv', delimiter=\",\", skiprows=2, nrows=61, encoding='utf-8', engine='python')\n",
    "paul = pd.read_csv('../Dataset/LoganPaul.csv', delimiter=\",\", skiprows=2, nrows=200, encoding='utf-8', engine='python')\n",
    "blogs = pd.read_csv('../Dataset/Kaggle.csv', delimiter=\",\", skiprows=2, encoding='latin-1', engine='python') \n",
    "tweets = pd.read_csv('../Dataset/twitter.csv', delimiter=\",\", skiprows=2, encoding='latin-1', engine='python') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.drop(['Topic', 'TweetId', \"TweetDate\"], axis = 1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_columns(data_frame):\n",
    "    data_frame = data_frame.iloc[:, :2]\n",
    "    data_frame.columns = ['label', 'comment']\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "okgo = fix_columns(okgo)\n",
    "trump = fix_columns(trump)\n",
    "swift = fix_columns(swift)\n",
    "royal = fix_columns(royal)\n",
    "paul = fix_columns(paul)\n",
    "tweets = fix_columns(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.label = tweets.label.replace({'positive': '1.0', 'negative':'-1.0', 'neutral': '0.0', 'irrelevant': '0.0'}, regex=True)\n",
    "tweets['label'] = pd.to_numeric(tweets['label'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = fix_columns(tweets)\n",
    "blogs = fix_columns(blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = pd.concat([okgo, trump, swift, royal, paul], ignore_index=True)\n",
    "data = videos.copy()\n",
    "data = fix_columns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.concat([videos, tweets, blogs], ignore_index=True)\n",
    "full = fix_columns(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"comment\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_str(DF):\n",
    "    DF[\"comment\"]= DF[\"comment\"].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_str(df)\n",
    "as_str(data)\n",
    "as_str(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner_fn(b):\n",
    "    for row in range(len(b)):\n",
    "        line = b.loc[row, \"comment\"]\n",
    "        b.loc[row,\"comment\"] = re.sub(\"[^a-zA-Z]\", \" \", line)\n",
    "cleaner_fn(df)\n",
    "cleaner_fn(data)\n",
    "cleaner_fn(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words, Lemmatization, Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_function(DF):\n",
    "    DF['com_token'] = DF['comment'].str.lower().str.split()\n",
    "    DF['com_remv'] = DF['com_token'].apply(lambda x: [y for y in x if y not in sw])\n",
    "    DF[\"com_lemma\"] = DF['com_remv'].apply(lambda x : [lemmatizer.lemmatize(y) for y in x]) # lemmatization\n",
    "    DF['com_stem'] = DF['com_lemma'].apply(lambda x : [ps.stem(y) for y in x]) # stemming\n",
    "    DF[\"com_tok_str\"] = DF[\"com_stem\"].apply(', '.join)\n",
    "    DF[\"com_full\"] = DF[\"com_stem\"].apply(' '.join)\n",
    "    #DF[\"com_tagged\"] = DF['comment'].apply(lambda x : [nltk.pos_tag(y) for y in x]) #word tagging\n",
    "    return DF\n",
    "df = nlp_function(df)\n",
    "data = nlp_function(data)\n",
    "full= nlp_function(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "full = full.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vectors(vectorizer,  x_train, x_test):\n",
    "    x_train = vectorizer.fit_transform(x_train)\n",
    "    x_test = vectorizer.transform(x_test)\n",
    "    return (x_train, x_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_data = CountVectorizer()\n",
    "count_vectorizer_full = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data, x_test_data, y_train_data, y_test_data = train_test_split(data['com_full'], data['label'], test_size=0.2, shuffle=True)\n",
    "x_train_data, x_test_data = word_to_vectors(count_vectorizer_data, x_train_data, x_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full, x_test_full, y_train_full, y_test_full = train_test_split(full['com_full'], full['label'], test_size=0.2, shuffle=True)\n",
    "x_train_full, x_test_full = word_to_vectors(count_vectorizer_full, x_train_full, x_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_data = LogisticRegression(solver='sag', max_iter=100, random_state=42, multi_class=\"multinomial\") \n",
    "lr_full = LogisticRegression(solver='sag', max_iter=100, random_state=42, multi_class=\"multinomial\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6394686907020873"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_data.fit(x_train_data, y_train_data)\n",
    "accuracy_score(y_test_data, lr_data.predict(x_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8672059319177621"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_full.fit(x_train_full, y_train_full)\n",
    "accuracy_score(y_test_full, lr_full.predict(x_test_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_data = SVC()\n",
    "svm_full = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6660341555977229"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_data.fit(x_train_data, y_train_data)\n",
    "accuracy_score(y_test_data, svm_data.predict(x_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8597910347152006"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_full.fit(x_train_full, y_train_full)\n",
    "accuracy_score(y_test_full, svm_full.predict(x_test_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_data = KNeighborsClassifier()\n",
    "knn_full = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5635673624288425"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_data.fit(x_train_data, y_train_data)\n",
    "accuracy_score(y_test_data, knn_data.predict(x_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8378833838894506"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_full.fit(x_train_full, y_train_full)\n",
    "accuracy_score(y_test_full, knn_full.predict(x_test_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier_data = RandomForestClassifier(n_estimators=10, random_state=10)\n",
    "random_forest_classifier_full = RandomForestClassifier(n_estimators=10, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5920303605313093"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_classifier_data.fit(x_train_data, y_train_data)\n",
    "accuracy_score(y_test_data, random_forest_classifier_data.predict(x_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.847994607347489"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_classifier_full.fit(x_train_full, y_train_full)\n",
    "accuracy_score(y_test_full, random_forest_classifier_full.predict(x_test_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_data = XGBClassifier()\n",
    "xgb_full = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got [-1.  0.  1.]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mxgb_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m accuracy_score(y_test_data, xgb_data\u001b[38;5;241m.\u001b[39mpredict(x_test_data))\n",
      "File \u001b[1;32mc:\\Users\\JB_SACHIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JB_SACHIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1491\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1486\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1488\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1489\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1490\u001b[0m ):\n\u001b[1;32m-> 1491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1492\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1493\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1494\u001b[0m     )\n\u001b[0;32m   1496\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got [-1.  0.  1.]"
     ]
    }
   ],
   "source": [
    "xgb_data.fit(x_train_data, y_train_data)\n",
    "accuracy_score(y_test_data, xgb_data.predict(x_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lr_data_full.pkl']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lr_data, 'lr_data.pkl')\n",
    "joblib.dump(lr_full, 'lr_data_full.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data.csv')\n",
    "full.to_csv('full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseSVC.predict() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[193], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msvm_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: BaseSVC.predict() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "svm_full.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_vectorizer_data.pkl']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(count_vectorizer_full, 'count_vectorizer_full.pkl')\n",
    "joblib.dump(count_vectorizer_data, 'count_vectorizer_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>com_token</th>\n",
       "      <th>com_remv</th>\n",
       "      <th>com_lemma</th>\n",
       "      <th>com_stem</th>\n",
       "      <th>com_tok_str</th>\n",
       "      <th>com_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Everyone knows brand s papers from  But  No on...</td>\n",
       "      <td>['everyone', 'knows', 'brand', 's', 'papers', ...</td>\n",
       "      <td>['everyone', 'knows', 'brand', 'papers', 'one'...</td>\n",
       "      <td>['everyone', 'know', 'brand', 'paper', 'one', ...</td>\n",
       "      <td>['everyon', 'know', 'brand', 'paper', 'one', '...</td>\n",
       "      <td>everyon, know, brand, paper, one, know, welfar...</td>\n",
       "      <td>everyon know brand paper one know welfar emplo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Your paper cut balance is</td>\n",
       "      <td>['your', 'paper', 'cut', 'balance', 'is']</td>\n",
       "      <td>['paper', 'cut', 'balance']</td>\n",
       "      <td>['paper', 'cut', 'balance']</td>\n",
       "      <td>['paper', 'cut', 'balanc']</td>\n",
       "      <td>paper, cut, balanc</td>\n",
       "      <td>paper cut balanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...</td>\n",
       "      <td>['oh', 'shit', 'when', 'i', 'saw', 'this', 'on...</td>\n",
       "      <td>['oh', 'shit', 'saw', 'front', 'page', 'love',...</td>\n",
       "      <td>['oh', 'shit', 'saw', 'front', 'page', 'love',...</td>\n",
       "      <td>['oh', 'shit', 'saw', 'front', 'page', 'love',...</td>\n",
       "      <td>oh, shit, saw, front, page, love, song</td>\n",
       "      <td>oh shit saw front page love song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Blowing my mind yet again</td>\n",
       "      <td>['blowing', 'my', 'mind', 'yet', 'again']</td>\n",
       "      <td>['blowing', 'mind', 'yet']</td>\n",
       "      <td>['blowing', 'mind', 'yet']</td>\n",
       "      <td>['blow', 'mind', 'yet']</td>\n",
       "      <td>blow, mind, yet</td>\n",
       "      <td>blow mind yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Should have gone with Dunder Mifflin</td>\n",
       "      <td>['should', 'have', 'gone', 'with', 'dunder', '...</td>\n",
       "      <td>['gone', 'dunder', 'mifflin']</td>\n",
       "      <td>['gone', 'dunder', 'mifflin']</td>\n",
       "      <td>['gone', 'dunder', 'mifflin']</td>\n",
       "      <td>gone, dunder, mifflin</td>\n",
       "      <td>gone dunder mifflin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>4983</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>He makes Americans look bad</td>\n",
       "      <td>['he', 'makes', 'americans', 'look', 'bad']</td>\n",
       "      <td>['makes', 'americans', 'look', 'bad']</td>\n",
       "      <td>['make', 'american', 'look', 'bad']</td>\n",
       "      <td>['make', 'american', 'look', 'bad']</td>\n",
       "      <td>make, american, look, bad</td>\n",
       "      <td>make american look bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>4984</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>OMG no stop Japan my fav STOPP</td>\n",
       "      <td>['omg', 'no', 'stop', 'japan', 'my', 'fav', 's...</td>\n",
       "      <td>['omg', 'stop', 'japan', 'fav', 'stopp']</td>\n",
       "      <td>['omg', 'stop', 'japan', 'fav', 'stopp']</td>\n",
       "      <td>['omg', 'stop', 'japan', 'fav', 'stopp']</td>\n",
       "      <td>omg, stop, japan, fav, stopp</td>\n",
       "      <td>omg stop japan fav stopp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>4985</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>This guy is making the U S look bad</td>\n",
       "      <td>['this', 'guy', 'is', 'making', 'the', 'u', 's...</td>\n",
       "      <td>['guy', 'making', 'u', 'look', 'bad']</td>\n",
       "      <td>['guy', 'making', 'u', 'look', 'bad']</td>\n",
       "      <td>['guy', 'make', 'u', 'look', 'bad']</td>\n",
       "      <td>guy, make, u, look, bad</td>\n",
       "      <td>guy make u look bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>4986</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>I thought Logan Paul was nicer than jake Paul ...</td>\n",
       "      <td>['i', 'thought', 'logan', 'paul', 'was', 'nice...</td>\n",
       "      <td>['thought', 'logan', 'paul', 'nicer', 'jake', ...</td>\n",
       "      <td>['thought', 'logan', 'paul', 'nicer', 'jake', ...</td>\n",
       "      <td>['thought', 'logan', 'paul', 'nicer', 'jake', ...</td>\n",
       "      <td>thought, logan, paul, nicer, jake, paul, wrong</td>\n",
       "      <td>thought logan paul nicer jake paul wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>4987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Can he do it front Yakuza</td>\n",
       "      <td>['can', 'he', 'do', 'it', 'front', 'yakuza']</td>\n",
       "      <td>['front', 'yakuza']</td>\n",
       "      <td>['front', 'yakuza']</td>\n",
       "      <td>['front', 'yakuza']</td>\n",
       "      <td>front, yakuza</td>\n",
       "      <td>front yakuza</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2548 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  label                                            comment  \\\n",
       "0              0   -1.0  Everyone knows brand s papers from  But  No on...   \n",
       "1              1    0.0         Your paper cut balance is                    \n",
       "2              2    1.0  OH SHIT WHEN I SAW THIS ON MY FRONT PAGE      ...   \n",
       "3              3    1.0                          Blowing my mind yet again   \n",
       "4              4    0.0               Should have gone with Dunder Mifflin   \n",
       "...          ...    ...                                                ...   \n",
       "2628        4983   -1.0                        He makes Americans look bad   \n",
       "2629        4984   -1.0                     OMG no stop Japan my fav STOPP   \n",
       "2630        4985   -1.0             This guy is making the U S look bad      \n",
       "2631        4986   -1.0  I thought Logan Paul was nicer than jake Paul ...   \n",
       "2632        4987    0.0                        Can he do it front Yakuza     \n",
       "\n",
       "                                              com_token  \\\n",
       "0     ['everyone', 'knows', 'brand', 's', 'papers', ...   \n",
       "1             ['your', 'paper', 'cut', 'balance', 'is']   \n",
       "2     ['oh', 'shit', 'when', 'i', 'saw', 'this', 'on...   \n",
       "3             ['blowing', 'my', 'mind', 'yet', 'again']   \n",
       "4     ['should', 'have', 'gone', 'with', 'dunder', '...   \n",
       "...                                                 ...   \n",
       "2628        ['he', 'makes', 'americans', 'look', 'bad']   \n",
       "2629  ['omg', 'no', 'stop', 'japan', 'my', 'fav', 's...   \n",
       "2630  ['this', 'guy', 'is', 'making', 'the', 'u', 's...   \n",
       "2631  ['i', 'thought', 'logan', 'paul', 'was', 'nice...   \n",
       "2632       ['can', 'he', 'do', 'it', 'front', 'yakuza']   \n",
       "\n",
       "                                               com_remv  \\\n",
       "0     ['everyone', 'knows', 'brand', 'papers', 'one'...   \n",
       "1                           ['paper', 'cut', 'balance']   \n",
       "2     ['oh', 'shit', 'saw', 'front', 'page', 'love',...   \n",
       "3                            ['blowing', 'mind', 'yet']   \n",
       "4                         ['gone', 'dunder', 'mifflin']   \n",
       "...                                                 ...   \n",
       "2628              ['makes', 'americans', 'look', 'bad']   \n",
       "2629           ['omg', 'stop', 'japan', 'fav', 'stopp']   \n",
       "2630              ['guy', 'making', 'u', 'look', 'bad']   \n",
       "2631  ['thought', 'logan', 'paul', 'nicer', 'jake', ...   \n",
       "2632                                ['front', 'yakuza']   \n",
       "\n",
       "                                              com_lemma  \\\n",
       "0     ['everyone', 'know', 'brand', 'paper', 'one', ...   \n",
       "1                           ['paper', 'cut', 'balance']   \n",
       "2     ['oh', 'shit', 'saw', 'front', 'page', 'love',...   \n",
       "3                            ['blowing', 'mind', 'yet']   \n",
       "4                         ['gone', 'dunder', 'mifflin']   \n",
       "...                                                 ...   \n",
       "2628                ['make', 'american', 'look', 'bad']   \n",
       "2629           ['omg', 'stop', 'japan', 'fav', 'stopp']   \n",
       "2630              ['guy', 'making', 'u', 'look', 'bad']   \n",
       "2631  ['thought', 'logan', 'paul', 'nicer', 'jake', ...   \n",
       "2632                                ['front', 'yakuza']   \n",
       "\n",
       "                                               com_stem  \\\n",
       "0     ['everyon', 'know', 'brand', 'paper', 'one', '...   \n",
       "1                            ['paper', 'cut', 'balanc']   \n",
       "2     ['oh', 'shit', 'saw', 'front', 'page', 'love',...   \n",
       "3                               ['blow', 'mind', 'yet']   \n",
       "4                         ['gone', 'dunder', 'mifflin']   \n",
       "...                                                 ...   \n",
       "2628                ['make', 'american', 'look', 'bad']   \n",
       "2629           ['omg', 'stop', 'japan', 'fav', 'stopp']   \n",
       "2630                ['guy', 'make', 'u', 'look', 'bad']   \n",
       "2631  ['thought', 'logan', 'paul', 'nicer', 'jake', ...   \n",
       "2632                                ['front', 'yakuza']   \n",
       "\n",
       "                                            com_tok_str  \\\n",
       "0     everyon, know, brand, paper, one, know, welfar...   \n",
       "1                                    paper, cut, balanc   \n",
       "2                oh, shit, saw, front, page, love, song   \n",
       "3                                       blow, mind, yet   \n",
       "4                                 gone, dunder, mifflin   \n",
       "...                                                 ...   \n",
       "2628                          make, american, look, bad   \n",
       "2629                       omg, stop, japan, fav, stopp   \n",
       "2630                            guy, make, u, look, bad   \n",
       "2631     thought, logan, paul, nicer, jake, paul, wrong   \n",
       "2632                                      front, yakuza   \n",
       "\n",
       "                                               com_full  \n",
       "0     everyon know brand paper one know welfar emplo...  \n",
       "1                                      paper cut balanc  \n",
       "2                      oh shit saw front page love song  \n",
       "3                                         blow mind yet  \n",
       "4                                   gone dunder mifflin  \n",
       "...                                                 ...  \n",
       "2628                             make american look bad  \n",
       "2629                           omg stop japan fav stopp  \n",
       "2630                                guy make u look bad  \n",
       "2631           thought logan paul nicer jake paul wrong  \n",
       "2632                                       front yakuza  \n",
       "\n",
       "[2548 rows x 9 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute '_validate_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12876\\3527661087.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_train_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'com_full'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\JB_SACHIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1463\u001b[0m                 \u001b[0mfit_method\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"partial_fit\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0m_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpartial_fit_and_fitted\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m                 \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m             with config_context(\n\u001b[0;32m   1470\u001b[0m                 skip_parameter_validation=(\n",
      "\u001b[1;32mc:\\Users\\JB_SACHIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute '_validate_params'"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "x_train_data = CountVectorizer.fit_transform(pd.DataFrame(data['com_full']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_data.predict(count_vectorizer_data.transform(['Bad video hate ']))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvf = joblib.load('count_vectorizer_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_full.predict(cvf.transform(['Good video']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 13493 features, but SVC is expecting 13536 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[207], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m             negative \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (positive, negative, neutral)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcompute_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGood Video\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[207], line 25\u001b[0m, in \u001b[0;36mcompute_results\u001b[1;34m(comments)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m comments:\n\u001b[0;32m     24\u001b[0m     comment \u001b[38;5;241m=\u001b[39m clean(comment)\n\u001b[1;32m---> 25\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     27\u001b[0m         neutral \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\JB_SACHIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:814\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    812\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 814\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[1;32mc:\\Users\\JB_SACHIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:429\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    414\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32mc:\\Users\\JB_SACHIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:607\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    604\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[1;32m--> 607\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m    617\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32mc:\\Users\\JB_SACHIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\JB_SACHIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 13493 features, but SVC is expecting 13536 features as input."
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#  Load the model\n",
    "model = joblib.load('svm_full.pkl')\n",
    "vectorizer = joblib.load('count_vectorizer_full.pkl')\n",
    "\n",
    "def clean(s):\n",
    "    s = re.sub(\"[^a-zA-Z]\", \" \", s)\n",
    "    words = s.split()\n",
    "    rem_sw = [word for word in words if word not in sw]\n",
    "    lemm_words = [lemmatizer.lemmatize(word) for word in rem_sw]\n",
    "    return ' '.join(lemm_words)\n",
    "\n",
    "\n",
    "def compute_results(comments):\n",
    "    positive, negative, neutral = (0, 0, 0)\n",
    "    for comment in comments:\n",
    "        comment = clean(comment)\n",
    "        output = model.predict(vectorizer.transform([comment]))[0]\n",
    "        if output == 0:\n",
    "            neutral += 1\n",
    "        elif output == 1:\n",
    "            positive += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "    return (positive, negative, neutral)\n",
    "print(compute_results(['Good Video']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(svm_full, 'svm_full.pkl')\n",
    "model = joblib.load('svm_full.pkl')\n",
    "# svm_full.predict(vectorizer.transform(['Good Video']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vectorizer.transform(['Good Video']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_count_vectorizer_full.pkl']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_full, 'saved_svm_full.pkl')\n",
    "joblib.dump(count_vectorizer_full, 'saved_count_vectorizer_full.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('saved_svm_full.pkl')\n",
    "vectorizer = joblib.load('saved_count_vectorizer_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vectorizer.transform(['Good Video']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
